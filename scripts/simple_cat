#!/usr/bin/env python
"""Replacement tool for the `ncecat` operators which allows you to explicitly
set the values of a concatenated dimension.

EXAMPLE
-------

Given the files "a.nc", "b.nc", "c.nc" and "d.nc", you can concatenate them
along a dimension called "letter" with the values a-d by executing:

    $ ./simple_cat --files a.nc b.nc c.nc d.nc --vals a b c d
    \              --dim letter letters.nc

If "--vals" is not passed, then the record dimension will be sequentially
numbered, starting at 0. If "--dim" is not passed, this record dimension will
simply be named "record." Globbing from the shell is supported.

NOTES
-----

- Explicitly disable decoding "times" when loading in files using xarray; this
  is to try to help avoid weird errors where xarray can't successfully load a
  dataset with no true index coordinate (as of xarray v0.6.1)

"""
import logging
logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger()

import sys
import marc_analysis as ma
import pandas as pd
import xarray

from argparse import ArgumentParser, RawDescriptionHelpFormatter
parser = ArgumentParser(description=__doc__,
                        formatter_class=RawDescriptionHelpFormatter)
parser.add_argument("out_fn", metavar="out.nc", type=str,
                    help="Name of output file with concatenated dimension")
parser.add_argument("--files", nargs="+", type=str, required=True,
                    help="Input files in the order they should be "
                         "concatenated")
parser.add_argument("--vals", nargs="+", type=str,
                    help="Values of the concatenated dimension in the order"
                         " corresponding to the input files")
parser.add_argument("--dim", type=str, default="record",
                    help="Name of the concatenated dimension")
parser.add_argument("--classic", action="store_true",
                    help="Save in netCDF3 classic format")
parser.add_argument("--debug", action='store_true',
                    help="Enable debug logging")

if __name__ == "__main__":

    args = parser.parse_args()

    if args.debug:
        logger.setLevel(logging.DEBUG)
        logger.debug("Debug logging enabled")

    if args.vals:
        logger.debug("Vals were passed - {}".format(args.vals))
        if len(args.files) != len(args.vals):
            raise Exception(
                "The same number of files and arguments must be given"
            )
        vals = args.vals
    else:
        logger.debug("No vals; defaulting to integer range")
        vals = range(len(args.files))

    if args.debug:
        logger.debug("{:>12s} | file".format(args.dim))
        logger.debug("-------------|-----")
        for fn, val in zip(args.files, vals):
            logger.debug("{:>12} | {}".format(val, fn))
    new_dim = pd.Index(vals, name=args.dim)

    # Load Datasets
    all_ds = []
    logger.debug("Loading files...")
    for fn in args.files:
        logger.debug("   " + fn)
        ds = xarray.open_dataset(fn, decode_times=False, decode_cf=False)
        all_ds.append(ds)

    # Concatenate
    logger.debug("Concatenating...")
    concat_ds = xarray.concat([ds.T for ds in all_ds], dim=new_dim)
    concat_ds = concat_ds.T

    # Append history
    call_str = __file__ + " " + " ".join(sys.argv[1:])
    concat_ds = ma.append_history(concat_ds, call_str)

    # Save to disk
    out_kwargs = {}
    if args.classic:
        out_kwargs['format'] = "NETCDF3_CLASSIC"
    logger.debug("Saving to {}".format(args.out_fn))
    concat_ds.to_netcdf(args.out_fn, **out_kwargs)
